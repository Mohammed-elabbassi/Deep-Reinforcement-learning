{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af4700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAP PERSONNALISÉE ---\n",
    "custom_map = [\n",
    "    \"SFFFFFFH\",  # (1,1)=Start, (1,8)=Hole\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFG\",  # (8,8)=Goal\n",
    "]\n",
    "\n",
    "# --- WRAPPER POUR RÉCOMPENSES PERSONNALISÉES ---\n",
    "class CustomFrozenLakeWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reward_win = +10\n",
    "        self.reward_lose = -10\n",
    "        self.reward_step = -1\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        if terminated:\n",
    "            custom_reward = self.reward_win if reward == 1 else self.reward_lose\n",
    "        else:\n",
    "            custom_reward = self.reward_step\n",
    "        return obs, custom_reward, terminated, truncated, info\n",
    "\n",
    "# --- SARSA ---\n",
    "def run(episodes, is_training=True, render=False):\n",
    "    env = gym.make('FrozenLake-v1', desc=custom_map, is_slippery=False, render_mode='human' if render else None)\n",
    "    env = CustomFrozenLakeWrapper(env)\n",
    "\n",
    "    if is_training:\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    else:\n",
    "        with open('custom_frozenlake_sarsa.pkl', 'rb') as f:\n",
    "            q = pickle.load(f)\n",
    "\n",
    "    learning_rate_a = 0.01\n",
    "    discount_factor_g = 0.9\n",
    "    epsilon = 0.1\n",
    "    epsilon_decay_rate = 0.0001\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Choisir la première action selon la politique ε-greedy\n",
    "        if is_training and rng.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q[state, :])\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            # Exécuter l'action et observer le résultat\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Choisir la prochaine action selon la même politique (ε-greedy)\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = np.argmax(q[new_state, :])\n",
    "\n",
    "            # Mise à jour SARSA : Q(s,a) ← Q(s,a) + α [R + γ Q(s',a') - Q(s,a)]\n",
    "            if is_training:\n",
    "                q[state, action] = q[state, action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * q[new_state, next_action] - q[state, action]\n",
    "                )\n",
    "\n",
    "            # Passer à l'état suivant et à l'action suivante\n",
    "            state = new_state\n",
    "            action = next_action\n",
    "\n",
    "        # Décroissance de epsilon\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "        if epsilon == 0:\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        # Enregistrer si on a gagné\n",
    "        if reward == 10:  # GAGNE = +10\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    \n",
    "\n",
    "    if is_training:\n",
    "        with open(\"custom_frozenlake_sarsa.pkl\", \"wb\") as f:\n",
    "            pickle.dump(q, f)\n",
    "\n",
    "# --- EXÉCUTION ---\n",
    "if __name__ == '__main__':\n",
    "    run(1, is_training=True, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CUSTOM WRAPPER ---\n",
    "class CustomFrozenLakeWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.win_state = 63   # Goal (bottom right)\n",
    "        self.lose_state = 7   # Hole at top right (1,8)\n",
    "        self.reward_win = +10\n",
    "        self.reward_lose = -10\n",
    "        self.reward_step = -1\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        if terminated:\n",
    "            if obs == self.win_state:\n",
    "                reward = self.reward_win\n",
    "            elif obs == self.lose_state:\n",
    "                reward = self.reward_lose\n",
    "        else:\n",
    "            reward = self.reward_step\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# --- SARSA FUNCTION ---\n",
    "def run(episodes, is_training=True, render=False):\n",
    "    # Créer l'environnement avec le wrapper\n",
    "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=False, render_mode='human' if render else None)\n",
    "    env = CustomFrozenLakeWrapper(env)\n",
    "\n",
    "    if is_training:\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))  # 64 x 4\n",
    "    else:\n",
    "        with open('custom_frozenlake_sarsa.pkl', 'rb') as f:\n",
    "            q = pickle.load(f)\n",
    "\n",
    "    learning_rate_a = 0.01\n",
    "    discount_factor_g = 0.9\n",
    "    epsilon = 0.1\n",
    "    epsilon_decay_rate = 0.0001\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Choisir la première action selon la politique ε-greedy\n",
    "        if is_training and rng.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q[state, :])\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            # Exécuter l'action et observer le résultat\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Choisir la prochaine action selon la même politique (ε-greedy)\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = np.argmax(q[new_state, :])\n",
    "\n",
    "            # Mise à jour SARSA : Q(s,a) ← Q(s,a) + α [R + γ Q(s',a') - Q(s,a)]\n",
    "            if is_training:\n",
    "                q[state, action] = q[state, action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * q[new_state, next_action] - q[state, action]\n",
    "                )\n",
    "\n",
    "            # Passer à l'état suivant et à l'action suivante\n",
    "            state = new_state\n",
    "            action = next_action\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "        if epsilon == 0:\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        if reward == 10:  # GAGNE = +10\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    \n",
    "\n",
    "    if is_training:\n",
    "        with open(\"custom_frozenlake_sarsa.pkl\", \"wb\") as f:\n",
    "            pickle.dump(q, f)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    # Test avec rendu visuel\n",
    "    run(10, is_training=True, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb61e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
