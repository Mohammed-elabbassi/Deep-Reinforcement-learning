{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4374eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca992d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomFrozenLakeWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.win_state = 63   # Goal (bottom right)\n",
    "        self.lose_state = 7   # Hole at top right (1,8)\n",
    "        self.reward_win = +10\n",
    "        self.reward_lose = -10\n",
    "        self.reward_step = -1\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        if terminated:\n",
    "            if obs == self.win_state:\n",
    "                reward = self.reward_win\n",
    "            elif obs == self.lose_state:\n",
    "                reward = self.reward_lose\n",
    "        else:\n",
    "            reward = self.reward_step\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# --- Q-LEARNING FUNCTION ---\n",
    "def run(episodes, is_training=True, render=False):\n",
    "    # Créer l'environnement avec le wrapper\n",
    "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=False, render_mode='human' if render else None)\n",
    "    env = CustomFrozenLakeWrapper(env)\n",
    "\n",
    "    if is_training:\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))  # 64 x 4\n",
    "    else:\n",
    "        with open('custom_frozenlake.pkl', 'rb') as f:\n",
    "            q = pickle.load(f)\n",
    "\n",
    "    learning_rate_a = 0.01\n",
    "    discount_factor_g = 0.9\n",
    "    epsilon = 0.1\n",
    "    epsilon_decay_rate = 0.0001\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q[state, :])\n",
    "\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if is_training:\n",
    "                q[state, action] = q[state, action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * np.max(q[new_state, :]) - q[state, action]\n",
    "                )\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            if render:\n",
    "                # Le rendu visuel est géré par l'environnement original\n",
    "                pass  # render_mode='human' s'occupe de tout\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "        if epsilon == 0:\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        if reward == 10:  #  GAGNE = +10\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "    if is_training:\n",
    "        with open(\"custom_frozenlake.pkl\", \"wb\") as f:\n",
    "            pickle.dump(q, f)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    # Entraînement avec rendu visuel\n",
    "    # run(15000, is_training=True, render=False)\n",
    "\n",
    "    # Test avec rendu visuel\n",
    "    run(10, is_training=True, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAP PERSONNALISÉE ---\n",
    "custom_map = [\n",
    "    \"SFFFFFFH\",  # (1,1)=Start, (1,8)=Hole\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFF\",\n",
    "    \"FFFFFFFG\",  # (8,8)=Goal\n",
    "]\n",
    "\n",
    "# --- WRAPPER POUR RÉCOMPENSES PERSONNALISÉES ---\n",
    "class CustomFrozenLakeWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reward_win = +10\n",
    "        self.reward_lose = -10\n",
    "        self.reward_step = -1\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        if terminated:\n",
    "            custom_reward = self.reward_win if reward == 1 else self.reward_lose\n",
    "        else:\n",
    "            custom_reward = self.reward_step\n",
    "        return obs, custom_reward, terminated, truncated, info\n",
    "\n",
    "# --- Q-LEARNING ---\n",
    "def run(episodes, is_training=True, render=False):\n",
    "    env = gym.make('FrozenLake-v1', desc=custom_map, is_slippery=False, render_mode='human' if render else None)\n",
    "    env = CustomFrozenLakeWrapper(env)\n",
    "\n",
    "    if is_training:\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    else:\n",
    "        with open('custom_frozenlake.pkl', 'rb') as f:\n",
    "            q = pickle.load(f)\n",
    "\n",
    "    learning_rate_a = 0.01\n",
    "    discount_factor_g = 0.9\n",
    "    epsilon = 0.1\n",
    "    epsilon_decay_rate = 0.0001\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q[state, :])\n",
    "\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if is_training:\n",
    "                q[state, action] = q[state, action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * np.max(q[new_state, :]) - q[state, action]\n",
    "                )\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "        if epsilon == 0:\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        if reward == 10:  # GAGNE = +10\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    if is_training:\n",
    "        with open(\"custom_frozenlake.pkl\", \"wb\") as f:\n",
    "            pickle.dump(q, f)\n",
    "\n",
    "# --- EXÉCUTION ---\n",
    "if __name__ == '__main__':\n",
    " \n",
    "    run(1, is_training=True, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2e4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
